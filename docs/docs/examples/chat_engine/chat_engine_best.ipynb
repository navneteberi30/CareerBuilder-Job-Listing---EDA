{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d89ae951",
      "metadata": {
        "id": "d89ae951"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/chat_engine/chat_engine_best.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f59565a0-62c0-4048-8a7c-e60fba161cd2",
      "metadata": {
        "id": "f59565a0-62c0-4048-8a7c-e60fba161cd2"
      },
      "source": [
        "# Chat Engine - Best Mode"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "433ea1f0-86e8-4912-8db8-dfe3d6a00b6d",
      "metadata": {
        "id": "433ea1f0-86e8-4912-8db8-dfe3d6a00b6d"
      },
      "source": [
        "The default chat engine mode is \"best\", which uses the \"openai\" mode if you are using an OpenAI model that supports the latest function calling API, otherwise uses the \"react\" mode"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a29935d",
      "metadata": {
        "id": "3a29935d"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97eea6c2",
      "metadata": {
        "id": "97eea6c2"
      },
      "outputs": [],
      "source": [
        "%pip install llama-index-llms-anthropic\n",
        "%pip install llama-index-llms-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6733b9ad",
      "metadata": {
        "id": "6733b9ad"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38a1e5ab",
      "metadata": {
        "id": "38a1e5ab"
      },
      "source": [
        "## Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4950aa4a",
      "metadata": {
        "id": "4950aa4a",
        "outputId": "d2241a15-8c17-4982-841e-ca15cd2a2107"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-01-27 12:15:55--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8001::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8001::154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75042 (73K) [text/plain]\n",
            "Saving to: â€˜data/paul_graham/paul_graham_essay.txtâ€™\n",
            "\n",
            "data/paul_graham/pa 100%[===================>]  73.28K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2024-01-27 12:15:55 (9.38 MB/s) - â€˜data/paul_graham/paul_graham_essay.txtâ€™ saved [75042/75042]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p 'data/paul_graham/'\n",
        "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b036854-2947-4ce9-b662-a17494f659b0",
      "metadata": {
        "id": "9b036854-2947-4ce9-b662-a17494f659b0"
      },
      "source": [
        "### Get started in 5 lines of code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "600cf600-c0d4-4f15-9424-0a2b07c34e77",
      "metadata": {
        "id": "600cf600-c0d4-4f15-9424-0a2b07c34e77"
      },
      "source": [
        "Load data and build index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e633c998-2d63-41c5-a678-46d7683e324a",
      "metadata": {
        "id": "e633c998-2d63-41c5-a678-46d7683e324a"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.llms.anthropic import Anthropic\n",
        "\n",
        "llm = OpenAI(model=\"gpt-4\")\n",
        "data = SimpleDirectoryReader(input_dir=\"./data/paul_graham/\").load_data()\n",
        "index = VectorStoreIndex.from_documents(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bba0968-611f-465b-b98e-73eb8a5bc4e4",
      "metadata": {
        "id": "6bba0968-611f-465b-b98e-73eb8a5bc4e4"
      },
      "source": [
        "Configure chat engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "938190e1-bb7c-4dda-895e-f8351026f8d9",
      "metadata": {
        "id": "938190e1-bb7c-4dda-895e-f8351026f8d9"
      },
      "outputs": [],
      "source": [
        "chat_engine = index.as_chat_engine(chat_mode=\"best\", llm=llm, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b13bb12d-b583-4f35-bede-6d95a023a2fd",
      "metadata": {
        "id": "b13bb12d-b583-4f35-bede-6d95a023a2fd"
      },
      "source": [
        "Chat with your data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "301220a3-0369-4104-bbbb-7bd084b793fc",
      "metadata": {
        "id": "301220a3-0369-4104-bbbb-7bd084b793fc",
        "outputId": "597dbc25-be80-40cc-8490-9ecb3ab6b2e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: What are the first programs Paul Graham tried writing?\n",
            "=== Calling Function ===\n",
            "Calling function: query_engine_tool with args: {\n",
            "  \"input\": \"What are the first programs Paul Graham tried writing?\"\n",
            "}\n",
            "Got output: The first programs Paul Graham tried writing were on the IBM 1401 that their school district used for what was then called \"data processing.\" The language he used was an early version of Fortran.\n",
            "========================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = chat_engine.chat(\n",
        "    \"What are the first programs Paul Graham tried writing?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "50051bbf-ef40-4be0-bb74-b42e944a4c38",
      "metadata": {
        "id": "50051bbf-ef40-4be0-bb74-b42e944a4c38",
        "outputId": "a7ebdf52-52bf-4c0c-afe2-93114d1915a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'response' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ef8b248111bc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'response' is not defined"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def get_db_name(promo_name):\n",
        "    promo_name = promo_name.replace('nz', 'au')\n",
        "    print (f\"this is promo region:{promo_name.split('-')[0]}\")\n",
        "    print (f\"this is promo_env :{promo_name.split('-')[1]}\")\n",
        "    name_parts = promo_name.split('-')\n",
        "\n",
        "    # Assuming the format is COUNTRY-ENV-YEAR-PROMOTION_BASE\n",
        "    if len(name_parts) < 4:\n",
        "        return \"Invalid promo name format\"\n",
        "\n",
        "    country = name_parts[0].upper()\n",
        "    env = name_parts[1].upper()\n",
        "    year_part = name_parts[2]\n",
        "    promo_name_base = '_'.join(name_parts[3:]).upper()\n",
        "\n",
        "    # Extract year from the year_part\n",
        "    year_match = re.search(r'\\d{2}$', year_part)\n",
        "    year = year_match.group(0) if year_match else \"\"\n",
        "\n",
        "    if env == 'PRD':\n",
        "        env = 'PROD'\n",
        "\n",
        "    # Construct the final database name\n",
        "    return f\"{env}_DB_IWIN_{country}_MCD_{promo_name_base}_20{year}\"\n",
        "\n",
        "# Example usage:\n",
        "print(get_db_name('UK-STG-25-SIDEQUESTS'))\n",
        "# Expected output: STG_DB_IWIN_UK_MCD_SIDEQUESTS_2025"
      ],
      "metadata": {
        "id": "ob5zbNLPsfFK",
        "outputId": "c39fbf8d-d1bd-4cd9-e096-c8646c038e72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ob5zbNLPsfFK",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is promo region:UK\n",
            "this is promo_env :STG\n",
            "STG_DB_IWIN_UK_MCD_SIDEQUESTS_2025\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}